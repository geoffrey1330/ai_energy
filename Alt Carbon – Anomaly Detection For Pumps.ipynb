{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5tJKndiTv429"
   },
   "source": [
    "# Alt Carbon: Anomaly Detection for Pumps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ln6skfB8FTaY"
   },
   "outputs": [],
   "source": [
    "BASE_DIR = \"/content/drive/MyDrive/DSN_AI/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a5kAFsPH7IBG"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KsaFxGMNnUgM"
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "from pylab import plot, show, figure, imshow, xlabel, ylabel, title\n",
    " \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    " \n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import sys\n",
    "import itertools\n",
    "import joblib\n",
    "import json\n",
    "import math \n",
    "import random\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    " \n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    " \n",
    "s =42\n",
    "np.random.seed(s)\n",
    "tf.random.set_seed(s)\n",
    "random.seed(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4078,
     "status": "ok",
     "timestamp": 1630828102296,
     "user": {
      "displayName": "Fortune Adekogbe",
      "photoUrl": "",
      "userId": "16801038909275339798"
     },
     "user_tz": -60
    },
    "id": "TRTt6QoV6-Sy",
    "outputId": "c9898660-0e7a-4fb6-ff35-90c33ebeee18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\r",
      "\u001b[K     |███▍                            | 10 kB 25.8 MB/s eta 0:00:01\r",
      "\u001b[K     |██████▊                         | 20 kB 26.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████                      | 30 kB 12.5 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▍                  | 40 kB 9.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▊               | 51 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████            | 61 kB 5.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▍        | 71 kB 6.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▊     | 81 kB 6.7 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▏ | 92 kB 6.5 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 97 kB 3.6 MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import kerastuner as kt\n",
    "except:\n",
    "    !pip install -q -U keras-tuner\n",
    "    import kerastuner as kt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oWUu43o0PLVa"
   },
   "source": [
    "## Extracting Audio Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1h-n2CfYINd"
   },
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K3KCB6DKYFWy"
   },
   "outputs": [],
   "source": [
    "\n",
    "# get machine IDs\n",
    "def get_section_names(target_dir,\n",
    "                      dir_name,\n",
    "                      ext=\"wav\"):\n",
    "    \"\"\"\n",
    "    target_dir : str\n",
    "        base directory path\n",
    "    dir_name : str\n",
    "        sub directory name\n",
    "    ext : str (default=\"wav)\n",
    "        file extension of audio files\n",
    "    return :\n",
    "        section_names : list [ str ]\n",
    "            list of section names extracted from the names of audio files\n",
    "    \"\"\"\n",
    "    # create test files\n",
    "    file_paths = os.listdir(f\"{target_dir}/{dir_name}\")\n",
    "    section_names = sorted(list(set(itertools.chain.from_iterable(\n",
    "        [re.findall('section_[0-9][0-9]', ext_id) for ext_id in file_paths]))))\n",
    "    return section_names\n",
    "\n",
    "# get the list of wave file paths\n",
    "def file_list_generator(target_dir,\n",
    "                        section_name,\n",
    "                        dir_name,\n",
    "                        mode=None,\n",
    "                        prefix_normal=\"normal\",\n",
    "                        prefix_anomaly=\"anomaly\",\n",
    "                        ext=\"wav\"):\n",
    "    \"\"\"\n",
    "    target_dir : str\n",
    "        base directory path\n",
    "    section_name : str\n",
    "        section name of audio file in <<dir_name>> directory\n",
    "    dir_name : str\n",
    "        sub directory name\n",
    "    prefix_normal : str (default=\"normal\")\n",
    "        normal directory name\n",
    "    prefix_anomaly : str (default=\"anomaly\")\n",
    "        anomaly directory name\n",
    "    ext : str (default=\"wav\")\n",
    "        file extension of audio files\n",
    "    return :\n",
    "        if the mode is \"development\":\n",
    "            files : list [ str ]\n",
    "                audio file list\n",
    "            labels : list [ boolean ]\n",
    "                label info. list\n",
    "                * normal/anomaly = 0/1\n",
    "        if the mode is \"evaluation\":\n",
    "            files : list [ str ]\n",
    "                audio file list\n",
    "    \"\"\"\n",
    "    # development\n",
    "    if mode:\n",
    "        query = os.path.abspath(\"{target_dir}/{dir_name}/{section_name}_*_{prefix_normal}_*.{ext}\".format(target_dir=target_dir,\n",
    "                                                                                                     dir_name=dir_name,\n",
    "                                                                                                     section_name=section_name,\n",
    "                                                                                                     prefix_normal=prefix_normal,\n",
    "                                                                                                     ext=ext))\n",
    "        normal_files = sorted(glob.glob(query))\n",
    "        normal_labels = np.zeros(len(normal_files))\n",
    "\n",
    "        query = os.path.abspath(\"{target_dir}/{dir_name}/{section_name}_*_{prefix_normal}_*.{ext}\".format(target_dir=target_dir,\n",
    "                                                                                                     dir_name=dir_name,\n",
    "                                                                                                     section_name=section_name,\n",
    "                                                                                                     prefix_normal=prefix_anomaly,\n",
    "                                                                                                     ext=ext))\n",
    "        anomaly_files = sorted(glob.glob(query))\n",
    "        anomaly_labels = np.ones(len(anomaly_files))\n",
    "\n",
    "        files = np.concatenate((normal_files, anomaly_files), axis=0)\n",
    "        labels = np.concatenate((normal_labels, anomaly_labels), axis=0)\n",
    "\n",
    "    # evaluation\n",
    "    else:\n",
    "        query = os.path.abspath(\"{target_dir}/{dir_name}/{section_name}_*.{ext}\".format(target_dir=target_dir,\n",
    "                                                                                                     dir_name=dir_name,\n",
    "                                                                                                     section_name=section_name,\n",
    "                                                                                                     ext=ext))\n",
    "        files = sorted(glob.glob(query))\n",
    "        labels = None\n",
    "    return files, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbop48EQIJ_x"
   },
   "source": [
    "### MFCC Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z_eRIs8Wm3KH"
   },
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 22050\n",
    "DURATION = 10\n",
    "SAMPLES_PER_TRACK = SAMPLE_RATE * DURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P_jv6URFiA0F"
   },
   "outputs": [],
   "source": [
    "def extract_mfcc(file_path, n_mfcc=128, n_fft=1024, hop_length=512):\n",
    "  \"\"\" Extract MFCC data from a track for use in genre prediction\n",
    "  \"\"\"\n",
    "  signal, sr = librosa.load(file_path, sr=SAMPLE_RATE)\n",
    "  # process segments, extracting mfccs and storing data \n",
    "  print(f'processing mfcc for {file_path}')\n",
    "  mfcc = librosa.feature.mfcc(signal,\n",
    "                              #sr=SAMPLE_RATE,\n",
    "                              n_fft=n_fft,\n",
    "                              n_mfcc=n_mfcc,\n",
    "                              hop_length=hop_length, power=2.0\n",
    "                              )\n",
    "  mfcc = mfcc.T # A transpose\n",
    "\n",
    "  return mfcc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54Te155cPWp7"
   },
   "source": [
    "### Melspectogram Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nD_tSEBdPb_e"
   },
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 22050\n",
    " \n",
    "# feature extractor\n",
    "def file_to_vectors(file_name,\n",
    "                    n_mels=64,\n",
    "                    n_frames=5,\n",
    "                    n_fft=1024,\n",
    "                    hop_length=512,\n",
    "                    power=2.0):\n",
    "    \"\"\"\n",
    "    convert file_name to a vector array.\n",
    "    file_name : str\n",
    "        target .wav file\n",
    "    return : numpy.array( numpy.array( float ) )\n",
    "        vector array\n",
    "        * dataset.shape = (dataset_size, feature_vector_length)\n",
    "    \"\"\"\n",
    "    # calculate the number of dimensions\n",
    "    dims = n_mels * n_frames\n",
    " \n",
    "    # generate melspectrogram using librosa\n",
    "    signal, sr = librosa.load(file_name, sr=None, mono=True)\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=signal,\n",
    "                                                     sr=sr,\n",
    "                                                     n_fft=n_fft,\n",
    "                                                     hop_length=hop_length,\n",
    "                                                     n_mels=n_mels,\n",
    "                                                     power=power)\n",
    " \n",
    "    # convert melspectrogram to log mel energies\n",
    "    log_mel_spectrogram = 20.0 / power * np.log10(np.maximum(mel_spectrogram, sys.float_info.epsilon))\n",
    "    # calculate total vector size\n",
    "    n_vectors = len(log_mel_spectrogram[0, :]) - n_frames + 1\n",
    "    # skip too short clips\n",
    "    if n_vectors < 1:\n",
    "        return np.empty((0, dims))\n",
    " \n",
    "    # generate feature vectors by concatenating multiframes\n",
    "    vectors = np.zeros((n_vectors, dims))\n",
    "    for t in range(n_frames):\n",
    "        vectors[:, n_mels * t : n_mels * (t + 1)] = log_mel_spectrogram[:, t : t + n_vectors].T\n",
    " \n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TeNQEzklWV1U"
   },
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MG4JILM_7xHh"
   },
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "id": "sdcGRYboYUZy"
   },
   "outputs": [],
   "source": [
    " \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    " \n",
    "def get_dense_autoencoder(input_dim, lr):\n",
    "    \"\"\"\n",
    "    define the keras model\n",
    "    the model based on the simple dense auto encoder \n",
    "    (128*128*128*128*8*128*128*128*128)\n",
    "    \"\"\"\n",
    " \n",
    "    x = Input(shape=(input_dim,))\n",
    " \n",
    "    h = Dense(128)(x)\n",
    "    h = BatchNormalization()(h)\n",
    "    h = Activation('relu')(h)\n",
    " \n",
    "    h = Dense(128)(h)\n",
    "    h = BatchNormalization()(h)\n",
    "    h = Activation('relu')(h)\n",
    " \n",
    "    h = Dense(128)(h)\n",
    "    h = BatchNormalization()(h)\n",
    "    h = Activation('relu')(h)\n",
    " \n",
    "    h = Dense(128)(h)\n",
    "    h = BatchNormalization()(h)\n",
    "    h = Activation('relu')(h)\n",
    " \n",
    "    h = Dense(8)(h)\n",
    "    h = BatchNormalization()(h)\n",
    "    h = Activation('relu')(h)\n",
    "    \n",
    "    h = Dense(128)(h)\n",
    "    h = BatchNormalization()(h)\n",
    "    h = Activation('relu')(h)\n",
    " \n",
    "    h = Dense(128)(h)\n",
    "    h = BatchNormalization()(h)\n",
    "    h = Activation('relu')(h)\n",
    " \n",
    "    h = Dense(128)(h)\n",
    "    h = BatchNormalization()(h)\n",
    "    h = Activation('relu')(h)\n",
    " \n",
    "    h = Dense(128)(h)\n",
    "    h = BatchNormalization()(h)\n",
    "    h = Activation('relu')(h)\n",
    " \n",
    "    h = Dense(input_dim)(h)\n",
    " \n",
    "    model = Model(inputs=x, outputs=h)\n",
    " \n",
    "    model.compile(optimizer=keras.optimizers.Adam(lr=lr), \n",
    "                  loss='mean_squared_error')\n",
    " \n",
    "    return model\n",
    "def load_model(file_path):\n",
    "    return keras.models.load_model(file_path, compile=False)\n",
    " \n",
    "def clear_session():\n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "id": "6CMR0c9pZzVv"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "# import additional libraries\n",
    "import scipy.stats\n",
    "import joblib\n",
    " \n",
    "# visualizer\n",
    "class Visualizer(object):\n",
    "    def __init__(self):\n",
    "        import matplotlib.pyplot as plt\n",
    "        self.plt = plt\n",
    "        self.fig = self.plt.figure(figsize=(7, 5))\n",
    "        self.plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    " \n",
    "    def loss_plot(self, loss, val_loss):\n",
    "        \"\"\"\n",
    "        Plot loss curve.\n",
    "        loss : list [ float ]\n",
    "            training loss time series.\n",
    "        val_loss : list [ float ]\n",
    "            validation loss time series.\n",
    "        return   : None\n",
    "        \"\"\"\n",
    "        ax = self.fig.add_subplot(1, 1, 1)\n",
    "        ax.cla()\n",
    "        ax.plot(loss)\n",
    "        ax.plot(val_loss)\n",
    "        ax.set_title(\"Model loss\")\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        ax.legend([\"Train\", \"Validation\"], loc=\"upper right\")\n",
    " \n",
    "    def save_figure(self, name):\n",
    "        \"\"\"\n",
    "        Save figure.\n",
    "        name : str\n",
    "            save png file path.\n",
    "        return : None\n",
    "        \"\"\"\n",
    "        self.plt.savefig(name)\n",
    " \n",
    "# get data from the list for file paths\n",
    "def file_list_to_data(file_list,\n",
    "                      msg=\"calc...\",\n",
    "                      n_mels=64,\n",
    "                      n_frames=5,\n",
    "                      n_hop_frames=1,\n",
    "                      n_fft=1024,\n",
    "                      hop_length=512,\n",
    "                      power=2.0):\n",
    "    \"\"\"\n",
    "    convert the file_list to a vector array.\n",
    "    file_to_vector_array() is iterated, and the output vector array is concatenated.\n",
    "    file_list : list [ str ]\n",
    "        .wav filename list of dataset\n",
    "    msg : str ( default = \"calc...\" )\n",
    "        description for tqdm.\n",
    "        this parameter will be input into \"desc\" param at tqdm.\n",
    "    return : numpy.array( numpy.array( float ) )\n",
    "        data for training (this function is not used for test.)\n",
    "        * dataset.shape = (number of feature vectors, dimensions of feature vectors)\n",
    "    \"\"\"\n",
    "    # calculate the number of dimensions\n",
    "    dims = n_mels * n_frames\n",
    " \n",
    "    # iterate file_to_vector_array()\n",
    "    for idx in tqdm(range(len(file_list)), desc=msg):\n",
    "        vectors = file_to_vectors(file_list[idx],\n",
    "                                                n_mels=n_mels,\n",
    "                                                n_frames=n_frames,\n",
    "                                                n_fft=n_fft,\n",
    "                                                hop_length=hop_length,\n",
    "                                                power=power)\n",
    "        vectors = vectors[: : n_hop_frames, :]\n",
    "        if idx == 0:\n",
    "            data = np.zeros((len(file_list) * vectors.shape[0], dims), float)\n",
    "        data[vectors.shape[0] * idx : vectors.shape[0] * (idx + 1), :] = vectors\n",
    " \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uyg6SSIm6glJ"
   },
   "source": [
    "### Extract Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 274
    },
    "executionInfo": {
     "elapsed": 988562,
     "status": "ok",
     "timestamp": 1630271260848,
     "user": {
      "displayName": "Fortune Adekogbe",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhaFGywco_D4lnXwBNczuWyoUbMB6pHXt2dYLaBChs=s64",
      "userId": "01227453287241652492"
     },
     "user_tz": -60
    },
    "id": "M25w9IisrOBe",
    "outputId": "43c36463-40e6-4307-9887-a0f10e40d759"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================\n",
      "[1/1] /content/drive/MyDrive/DSN AI/Dev Data/dev_data_pump/pump\n",
      "============== DATASET_GENERATOR ==============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generate train_dataset: 100%|██████████| 1003/1003 [03:52<00:00,  4.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "(32096, 64, 128, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generate train_dataset: 100%|██████████| 1003/1003 [04:15<00:00,  3.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "(32096, 64, 128, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generate train_dataset: 100%|██████████| 1003/1003 [04:13<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "(32096, 64, 128, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 504x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# \"development\": mode == True\n",
    "# \"evaluation\": mode == False\n",
    "mode = True\n",
    " \n",
    "# load base_directory list\n",
    "dirs = [f\"{BASE_DIR}Dev Data/dev_data_pump/pump\"]\n",
    "model_dir = \"Models/MLP_Autoencoder\"\n",
    " \n",
    "# loop of the base directory\n",
    "for idx, target_dir in enumerate(dirs):\n",
    "    print(\"\\n===========================\")\n",
    "    print(f\"[{idx+1}/{len(dirs)}] {target_dir}\")\n",
    " \n",
    "    # set path\n",
    "    machine_type = os.path.split(target_dir)[1]\n",
    " \n",
    "    section_names_file_path = f\"{BASE_DIR}{model_dir}/section_names_{machine_type}.pkl\"\n",
    "    # pickle file for storing anomaly score distribution\n",
    "    \n",
    "    \n",
    "    # get section names from wave file names\n",
    "    section_names =get_section_names(target_dir, dir_name=\"train\")\n",
    "    unique_section_names = np.unique(section_names)\n",
    "    n_sections = unique_section_names.shape[0]\n",
    "    \n",
    "    # make condition dictionary\n",
    "    joblib.dump(unique_section_names, section_names_file_path)\n",
    " \n",
    "    # generate dataset\n",
    "    print(\"============== DATASET_GENERATOR ==============\")\n",
    "    # number of wave files in each section\n",
    "    # required for calculating y_pred for each wave file\n",
    "    n_files_ea_section = []\n",
    "        \n",
    "    for section_idx, section_name in enumerate(unique_section_names):\n",
    "        # get file list for each section\n",
    "        # all values of y_true are zero in training\n",
    "        files, y_true = file_list_generator(target_dir=target_dir,\n",
    "                                                section_name=section_name,\n",
    "                                                dir_name=\"train\",\n",
    "                                                mode=mode)\n",
    " \n",
    "        n_files_ea_section.append(len(files))\n",
    " \n",
    "        data = file_list_to_data(files,\n",
    "                                msg=\"generate train_dataset\",\n",
    "                                n_mels=128,\n",
    "                                n_frames=64,\n",
    "                                n_hop_frames=8,\n",
    "                                n_fft=1024,\n",
    "                                hop_length=512,\n",
    "                                power=2.0)\n",
    " \n",
    "        # number of all files\n",
    "        n_all_files = n_files_ea_section[section_idx]\n",
    "        # number of vectors for each wave file\n",
    "        n_vectors_ea_file = int(data.shape[0] / n_all_files)\n",
    "        print(n_vectors_ea_file)\n",
    "        \n",
    "        # make one-hot vector for conditioning\n",
    "        condition = np.ones((data.shape[0],2), float)\n",
    "        condition[:,1] = 1\n",
    "        n_vectors = n_vectors_ea_file * n_files_ea_section[section_idx]\n",
    " \n",
    "        # 1D vector to 2D image\n",
    "        data = data.reshape(data.shape[0], 64, 128, 1)\n",
    "        print(data.shape)\n",
    " \n",
    "        joblib.dump([data,condition], f\"{BASE_DIR}Dev Data/dev_data_pump/data_{section_name}.jl\", compress=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0KJ2Sk9834e"
   },
   "source": [
    "### Loading Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aPMZXgqZ1E33"
   },
   "outputs": [],
   "source": [
    "section_idx = 2\n",
    "section_name = f\"section_0{section_idx}\"\n",
    "machine_type = \"pump\"\n",
    "ae_architecture = \"lstm\"\n",
    "data, _ = joblib.load(f\"{BASE_DIR}Dev Data/dev_data_pump/data_{section_name}.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1667,
     "status": "ok",
     "timestamp": 1630828125694,
     "user": {
      "displayName": "Fortune Adekogbe",
      "photoUrl": "",
      "userId": "16801038909275339798"
     },
     "user_tz": -60
    },
    "id": "f2oG3aJ9s-SJ",
    "outputId": "901e069a-4c71-44d4-c8b1-76b891ad4d9a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32096, 64, 128)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if ae_architecture == \"dense\":\n",
    "    data = data.reshape(32096, -1)\n",
    "elif ae_architecture == \"lstm\":\n",
    "    data = data.reshape(data.shape[:-1])\n",
    "    data_norm = tf.keras.utils.normalize(data)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MKbNMrpPqyS"
   },
   "source": [
    "### Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "psodj6liO0_9"
   },
   "source": [
    "**Model Builders**\n",
    "\n",
    "These help with setting up the search space for the tuning proces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N1oNp3PZPtjJ"
   },
   "outputs": [],
   "source": [
    "def lstm_encoder_model_builder(hp):\n",
    "    # define model\n",
    "    timesteps = 64\n",
    "    n_features = 128\n",
    "    units1=hp.Int('units_1', 128, 2048, step=64)\n",
    "    units2=hp.Int('units_2', 64, 1024, step=64)\n",
    " \n",
    " \n",
    "    model = keras.Sequential()\n",
    "    model.add(LSTM(units1, activation='relu', input_shape=(timesteps,n_features), return_sequences=True))\n",
    "    model.add(LSTM(units2, activation='relu', return_sequences=False))\n",
    "    model.add(RepeatVector(timesteps))\n",
    "    model.add(LSTM(units2, activation='relu', return_sequences=True))\n",
    "    model.add(LSTM(units1, activation='relu', return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(n_features)))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    return model\n",
    "def lstm_model_builder(hp):\n",
    " \n",
    "    # define model\n",
    "    timesteps = 64\n",
    "    n_features = 128\n",
    "    units1=hp.Int('units_1', 128, 512, step=32)\n",
    "    units2=hp.Int('units_2', 128, 256, step=32)\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    model.add(LSTM(units1, activation='relu', input_shape=(timesteps,n_features), return_sequences=True))\n",
    "    #model.add(LSTM(units2, activation='relu', return_sequences=True))\n",
    "    model.add(LSTM(units2, activation='relu', return_sequences=True))\n",
    "    model.add(LSTM(units1, activation='relu', return_sequences=True))\n",
    "    model.add(Dense( n_features))\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    " \n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                loss='mean_squared_error',\n",
    "                metrics=['mse'])\n",
    "    return model    \n",
    "def dense_model_builder(hp):\n",
    "    input_dim = 8192\n",
    "    units1=hp.Int('units_1', 128, 2048, step=64)\n",
    "    units2=hp.Int('units_2', 128, 1024, step=64)\n",
    "    units3=hp.Int('units_3', 128, 512, step=64)\n",
    "    units4=hp.Int('units_4', 8, 32, step=8)        # create model\n",
    "    x = Input(shape=(input_dim,))\n",
    "    h = Dense(units1)(x)\n",
    "    h = BatchNormalization()(h)\n",
    "    h = Activation('relu')(h)\n",
    " \n",
    "    h = Dense(units2)(h)\n",
    "    h = BatchNormalization()(h)\n",
    "    h = Activation('relu')(h)\n",
    " \n",
    "    h = Dense(units3)(h)\n",
    "    h = BatchNormalization()(h)\n",
    "    h = Activation('relu')(h)\n",
    " \n",
    "    h = Dense(units4)(h)\n",
    "    h = BatchNormalization()(h)\n",
    "    h = Activation('relu')(h)\n",
    "    \n",
    "    h = Dense(units3)(h)\n",
    "    h = BatchNormalization()(h)\n",
    "    h = Activation('relu')(h)\n",
    " \n",
    "    h = Dense(units2)(h)\n",
    "    h = BatchNormalization()(h)\n",
    "    h = Activation('relu')(h)\n",
    " \n",
    "    h = Dense(units1)(h)\n",
    "    h = BatchNormalization()(h)\n",
    "    h = Activation('relu')(h)\n",
    " \n",
    "    h = Dense(input_dim)(h)\n",
    " \n",
    "    model = Model(inputs=x, outputs=h)\n",
    " \n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    " \n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate,  ),\n",
    "                loss='mean_squared_error',\n",
    "                metrics=['mse'])\n",
    " \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3hOkkdBERFbC"
   },
   "source": [
    "**Instantiate Bayesian Optimization Tuner**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M-sDk03XPtmE"
   },
   "outputs": [],
   "source": [
    "tuner = kt.tuners.bayesian.BayesianOptimization(lstm_model_builder,\n",
    "                                                objective='val_mse',\n",
    "                                                max_trials=5,\n",
    "                                                num_initial_points=2,\n",
    "                                                seed=s,\n",
    "                                                directory='.',\n",
    "                                                project_name=f'bayesian_{ae_architecture}',\n",
    "                                                overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZkeaFT-7PtpC"
   },
   "outputs": [],
   "source": [
    " tuner.search(x=data_norm,\n",
    "              y=data_norm,\n",
    "            epochs= 1,\n",
    "            shuffle=True,\n",
    "            batch_size=512,\n",
    "            validation_split=0.1           \n",
    "            )\n",
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    " \n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete.\"\"\")\n",
    "best_hps.get_config()['values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iQpjeFHaPtsN"
   },
   "outputs": [],
   "source": [
    " model = tuner.get_best_models()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RKdM0iiwPtup"
   },
   "outputs": [],
   "source": [
    "### Load Model\n",
    "\n",
    "model = keras.models.load_model('/content/drive/MyDrive/DSN_AI/Dev Data/dev_data_pump/pump/models/best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1bUf4LZUPtx4"
   },
   "outputs": [],
   "source": [
    "### Continue Training\n",
    "\n",
    "stop_early = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(\"/content/drive/MyDrive/DSN_AI/Dev Data/dev_data_pump/pump/models/best_model.h5\",\n",
    "                                            verbose = 0, monitor = \"val_mse\",\n",
    "                                            save_best_only=True,\n",
    "                                            mode=\"auto\", save_freq = 'epoch')\n",
    "# Train the RNN\n",
    "history = model.fit(x=data,\n",
    "              y=data,\n",
    "            epochs= 100,\n",
    "            shuffle=True,\n",
    "            validation_split=0.1,\n",
    "            callbacks=[stop_early,checkpoint]\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aqpSEE4sCBvd"
   },
   "source": [
    "### Fitting anomaly score distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FdDZnipZGm_U"
   },
   "outputs": [],
   "source": [
    "# calculate y_pred for fitting anomaly score distribution\n",
    "y_pred = []\n",
    "p = model.predict(data_norm)\n",
    "ps = p.reshape(1003,32,64,128)\n",
    "mfccs = data_norm.reshape(1003,32,64,128)\n",
    " \n",
    "for mfcc,p in zip(mfccs,ps):\n",
    "    y_pred.append(np.mean(np.square(mfcc - model.predict(p))))\n",
    " \n",
    "print(np.mean(y_pred))\n",
    "model_dir = \"Models/MLP_Autoencoder/pump\"\n",
    "model_file_path = f\"{BASE_DIR}{model_dir}/model_{machine_type}_{section_name}.hdf5\"\n",
    " \n",
    "# pickle file for storing section names\n",
    " \n",
    "score_distr_file_path = f\"{BASE_DIR}{model_dir}/score_distr_{machine_type}_{section_name}.pkl\"\n",
    "# fit anomaly score distribution\n",
    "shape_hat, loc_hat, scale_hat = scipy.stats.gamma.fit(y_pred)\n",
    "gamma_params = [shape_hat, loc_hat, scale_hat]\n",
    "print(gamma_params)\n",
    "joblib.dump(gamma_params, score_distr_file_path)\n",
    " \n",
    "model.save(model_file_path)\n",
    "print(f\"save_model -> {model_file_path}\")\n",
    "print(\"============== END TRAINING ==============\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9fQzaPI3N1-6"
   },
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "us-Eoa36P2jk"
   },
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "from sklearn import metrics\n",
    "import csv\n",
    "#machine type\n",
    "machine_type = \"pump\"\n",
    " \n",
    "# make output result directory\n",
    "result_directory = f\"{BASE_DIR}results/MLP_Autoencoder/{machine_type}\"\n",
    "# load base directory\n",
    "model_dir = \"Models/MLP_Autoencoder/pump\"\n",
    "dirs = [f\"/content/drive/MyDrive/DSN_AI/Dev Data/dev_data_pump/{machine_type}\"]\n",
    " \n",
    "section_name = f\"section_0{section_idx}\"\n",
    "# initialize lines in csv for AUC and pAUC\n",
    "n_mels = 128\n",
    "n_frames = 64\n",
    "n_hop_frames = 8\n",
    "n_fft = 10\n",
    "hop_length = 512\n",
    "power = 2.0\n",
    " \n",
    "def save_csv(save_file_path,\n",
    "             save_data):\n",
    "    with open(save_file_path, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f, lineterminator='\\n')\n",
    "        writer.writerows(save_data)\n",
    " \n",
    "performance_over_all = []\n",
    "csv_lines = []\n",
    "# loop of the base directory\n",
    "for idx, target_dir in enumerate(dirs):\n",
    "    print(\"\\n===========================\")\n",
    "    print(f\"[{idx+1}/{len(dirs)}] {target_dir}\")\n",
    "    machine_type = os.path.split(target_dir)[1]\n",
    " \n",
    "    print(\"============== MODEL LOAD ==============\")\n",
    "    # load model file\n",
    "    model_file = f\"{BASE_DIR}{model_dir}/model_{machine_type}_{section_name}.hdf5\"\n",
    "    if not os.path.exists(model_file):\n",
    "        print(f\"{machine_type} model not found \")\n",
    " \n",
    "    model = load_model(model_file)\n",
    "    model.summary()\n",
    " \n",
    "    # load section names for conditioning\n",
    "    section_names_file_path = f\"{BASE_DIR}{model_dir}/section_names_{machine_type}.pkl\"\n",
    "    n_sections = 3\n",
    "    \n",
    "    # load anomaly score distribution for determining threshold\n",
    "    score_distr_file_path = f\"{BASE_DIR}{model_dir}/score_distr_{machine_type}_{section_name}.pkl\"\n",
    "    shape_hat, loc_hat, scale_hat = joblib.load(score_distr_file_path)\n",
    " \n",
    "    # determine threshold for decision\n",
    "    decision_threshold = {0:0.999998768, 1: 0.57764,2:0.567035}\n",
    "    decision_threshold = scipy.stats.gamma.ppf(q=decision_threshold[section_idx], a=shape_hat, loc=loc_hat, scale=scale_hat)\n",
    "    print(decision_threshold)\n",
    " \n",
    "    # results for each machine type\n",
    "    csv_lines.append([machine_type])\n",
    "    csv_lines.append([\"section\", \"domain\", \"AUC\", \"pAUC\", \"precision\", \"recall\", \"F1 score\"])\n",
    "    performance = []\n",
    "    dir_names = [\"source_test\", \"target_test\"]\n",
    "    \n",
    "    for dir_name in dir_names:\n",
    " \n",
    "        #list machine id\n",
    "        section_names = get_section_names(target_dir, dir_name=dir_name)\n",
    " \n",
    "        section_name = section_names[section_idx]\n",
    " \n",
    "        # load test file\n",
    "        files, y_true = file_list_generator(target_dir=target_dir,\n",
    "                                                section_name=section_name,\n",
    "                                                dir_name=dir_name,\n",
    "                                                mode=True)\n",
    " \n",
    "        # setup anomaly score file path\n",
    "        anomaly_score_csv = f\"{result_directory}/anomaly_score_{machine_type}_{section_name}_{dir_name}.csv\"\n",
    "        anomaly_score_list = []\n",
    " \n",
    "        # setup decision result file path\n",
    "        decision_result_csv = f\"{result_directory}/decision_result_{machine_type}_{section_name}_{dir_name}.csv\"\n",
    " \n",
    "        decision_result_list = []\n",
    " \n",
    "        print(\"\\n============== BEGIN TEST FOR A SECTION ==============\")\n",
    "        y_pred = [0. for k in files]\n",
    "        pred = []\n",
    "        for file_idx, file_path in tqdm(enumerate(files), total=len(files)):\n",
    "            try:\n",
    "                data = file_to_vectors(file_path,\n",
    "                                           n_mels=n_mels,\n",
    "                                           n_frames=n_frames,\n",
    "                                           n_fft=n_fft,\n",
    "                                           hop_length=hop_length,\n",
    "                                           power=power)\n",
    "            except:\n",
    "                print(f\"File broken!!: {file_path}\")\n",
    " \n",
    "            data = tf.keras.utils.normalize(data).reshape(-1,64, 128)\n",
    "            p = model.predict(data)\n",
    "            y_pred[file_idx] = np.mean(np.square(data - p))\n",
    "            pred.append(np.mean(np.square(data - p)))\n",
    " \n",
    " \n",
    "            # store anomaly scores\n",
    "            anomaly_score_list.append([os.path.basename(file_path), y_pred[file_idx]])\n",
    " \n",
    "            \n",
    " \n",
    "            # store decision results\n",
    "            if y_pred[file_idx] > decision_threshold:\n",
    "                y_pred[file_idx] = 1\n",
    "                decision_result_list.append([os.path.basename(file_path), 1])\n",
    "            else:\n",
    "                y_pred[file_idx] = 0\n",
    "                decision_result_list.append([os.path.basename(file_path), 0])\n",
    "        \n",
    "        # output anomaly scores\n",
    "        save_csv(save_file_path=anomaly_score_csv, save_data=anomaly_score_list)\n",
    "        print(f\"anomaly score result ->  {anomaly_score_csv}\")\n",
    " \n",
    "        # output decision results\n",
    "        save_csv(save_file_path=decision_result_csv, save_data=decision_result_list)\n",
    "        print(f\"decision result ->  {decision_result_csv}\")\n",
    " \n",
    "        # append AUC and pAUC to lists\n",
    "        \n",
    "        print(metrics.accuracy_score(y_true, y_pred))\n",
    "        auc = metrics.roc_auc_score(y_true, y_pred)\n",
    "        p_auc = metrics.roc_auc_score(y_true, y_pred, max_fpr=0.1)\n",
    "        tn, fp, fn, tp = metrics.confusion_matrix(y_true, [1 if x > decision_threshold else 0 for x in y_pred]).ravel()\n",
    "        prec = tp / np.maximum(tp + fp, sys.float_info.epsilon)\n",
    "        recall = tp / np.maximum(tp + fn, sys.float_info.epsilon)\n",
    "        f1 = 2.0 * prec * recall / np.maximum(prec + recall, sys.float_info.epsilon)\n",
    "        csv_lines.append([section_name.split(\"_\", 1)[1], dir_name.split(\"_\", 1)[0], auc, p_auc, prec, recall, f1])\n",
    "        performance.append([auc, p_auc, prec, recall, f1])\n",
    "        performance_over_all.append([auc, p_auc, prec, recall, f1])\n",
    "        print(metrics.accuracy_score(y_true, y_pred))\n",
    "        print(f\"AUC : {auc}\")\n",
    "        print(f\"pAUC : {p_auc}\")\n",
    "        print(f\"precision : {prec}\")\n",
    "        print(f\"recall : {recall}\")\n",
    "        print(f\"F1 score : {f1}\")\n",
    " \n",
    "        print(\"\\n============ END OF TEST FOR A SECTION ============\")\n",
    " \n",
    " \n",
    "# calculate averages for AUCs and pAUCs\n",
    "amean_performance = np.mean(np.array(performance, dtype=float), axis=0)\n",
    "csv_lines.append([\"arithmetic mean\", \"\"] + list(amean_performance))\n",
    "hmean_performance = scipy.stats.hmean(np.maximum(np.array(performance, dtype=float), sys.float_info.epsilon), axis=0)\n",
    "csv_lines.append([\"harmonic mean\", \"\"] + list(hmean_performance))\n",
    "csv_lines.append([])\n",
    " \n",
    "clear_session()\n",
    "gc.collect()\n",
    " \n",
    "csv_lines.append([\"\", \"\", \"AUC\", \"pAUC\", \"precision\", \"recall\", \"F1 score\"])\n",
    "# calculate averages for AUCs and pAUCs\n",
    "amean_performance = np.mean(np.array(performance_over_all, dtype=float), axis=0)\n",
    "csv_lines.append([\"arithmetic mean over all machine types, sections, and domains\", \"\"] + list(amean_performance))\n",
    "hmean_performance = scipy.stats.hmean(np.maximum(np.array(performance_over_all, dtype=float), sys.float_info.epsilon), axis=0)\n",
    "csv_lines.append([\"harmonic mean over all machine types, sections, and domains\", \"\"] + list(hmean_performance))\n",
    "csv_lines.append([])\n",
    " \n",
    "# output results\n",
    "result_path = f\"{result_directory}/result.csv\"\n",
    "print(\"results -> {}\".format(result_path))\n",
    "save_csv(save_file_path=result_path, save_data=csv_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E25on2gFHsFU"
   },
   "source": [
    "Tuning the Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5117,
     "status": "ok",
     "timestamp": 1630830183320,
     "user": {
      "displayName": "Fortune Adekogbe",
      "photoUrl": "",
      "userId": "16801038909275339798"
     },
     "user_tz": -60
    },
    "id": "xSdPDJqnF-tC",
    "outputId": "5cf544be-a9a4-4c35-b24c-a349ba28938e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'thresh': 0.0037117979606261244, 'acc': 0.535, 'dt': 0.5670350000002023}\n"
     ]
    }
   ],
   "source": [
    "store = {\"thresh\":0, \"acc\":0, \"dt\":0}\n",
    "dt = 0.56\n",
    "while dt< 0.57:\n",
    "  decision_threshold = scipy.stats.gamma.ppf(q=dt, a=shape_hat, loc=loc_hat, scale=scale_hat)\n",
    "  y_pred = (np.array(pred)>decision_threshold).astype(int)\n",
    "  if metrics.accuracy_score(y_true,y_pred)> store[\"acc\"]:\n",
    "    store[\"acc\"] =metrics.accuracy_score(y_true,y_pred)\n",
    "    store[\"thresh\"] = decision_threshold\n",
    "    store[\"dt\"]  = dt\n",
    "  dt+=0.000001\n",
    " \n",
    "print(store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fLWUezToEgQ0"
   },
   "source": [
    "### Single Track Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cIOJUeWdEkDj"
   },
   "outputs": [],
   "source": [
    "#%%writefile /content/drive/MyDrive/DSN_AI/Models/MLP_Autoencoder/inference.py\n",
    "#import libraries\n",
    "import librosa\n",
    "import requests\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "import scipy.stats\n",
    " \n",
    "from cloud_client import *\n",
    " \n",
    "section_idx = 0\n",
    "machine_type = \"pump\"\n",
    "pump_container = f\"{machine_type}-section-0{section_idx}\"\n",
    "file_path = download_file(pump_container)\n",
    "section_name = f\"section_0{section_idx}\"\n",
    " \n",
    " \n",
    "# constants\n",
    "SAMPLE_RATE = 22050\n",
    " \n",
    "model_dir = f\"{BASE_DIR}Models/MLP_Autoencoder/{machine_type}\"\n",
    " \n",
    "# load anomaly score distribution for determining threshold\n",
    "score_distr_file_path = f\"{model_dir}/score_distr_{machine_type}_{section_name}.pkl\"\n",
    "shape_hat, loc_hat, scale_hat = joblib.load(score_distr_file_path)\n",
    " \n",
    "decision_threshold = {0: 0.999998768, 1:0.57764, 2:0.567035}\n",
    "decision_threshold = scipy.stats.gamma.ppf(q=decision_threshold[section_idx], a=shape_hat, loc=loc_hat, scale=scale_hat)\n",
    " \n",
    " \n",
    "# feature extractor\n",
    "def file_to_vectors(file_path,\n",
    "                    n_mels=64,\n",
    "                    n_frames=5,\n",
    "                    n_fft=1024,\n",
    "                    hop_length=512,\n",
    "                    power=2.0):\n",
    "    \"\"\"\n",
    "    convert file_name to a vector array.\n",
    "    file_name : str\n",
    "        target .wav file\n",
    "    return : numpy.array( numpy.array( float ) )\n",
    "        vector array\n",
    "        * dataset.shape = (dataset_size, feature_vector_length)\n",
    "    \"\"\"\n",
    "    # calculate the number of dimensions\n",
    "    dims = n_mels * n_frames\n",
    " \n",
    "    # generate melspectrogram using librosa\n",
    "    signal, sr = librosa.load(file_path, sr=None, mono=True)\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=signal,\n",
    "                                                     sr=sr,\n",
    "                                                     n_fft=n_fft,\n",
    "                                                     hop_length=hop_length,\n",
    "                                                     n_mels=n_mels,\n",
    "                                                     power=power)\n",
    " \n",
    "    # convert melspectrogram to log mel energies\n",
    "    log_mel_spectrogram = 20.0 / power * np.log10(np.maximum(mel_spectrogram, sys.float_info.epsilon))\n",
    "    # calculate total vector size\n",
    "    n_vectors = len(log_mel_spectrogram[0, :]) - n_frames + 1\n",
    "    # skip too short clips\n",
    "    if n_vectors < 1:\n",
    "        return np.empty((0, dims))\n",
    " \n",
    "    # generate feature vectors by concatenating multiframes\n",
    "    vectors = np.zeros((n_vectors, dims))\n",
    "    for t in range(n_frames):\n",
    "        vectors[:, n_mels * t : n_mels * (t + 1)] = log_mel_spectrogram[:, t : t + n_vectors].T\n",
    "    # delete audio file\n",
    "    os.remove(file_path)\n",
    "    return vectors\n",
    " \n",
    "# load model file\n",
    "model_file = f\"{model_dir}/model_{machine_type}_{section_name}.hdf5\"\n",
    "if not os.path.exists(model_file):\n",
    "    print(f\"{machine_type} model not found \")\n",
    " \n",
    "model = tf.keras.models.load_model(model_file)\n",
    " \n",
    "# make prediction\n",
    "data = file_to_vectors(file_path, 128, 64).reshape(-1,64,128)\n",
    " \n",
    "data_norm = tf.keras.utils.normalize(data)\n",
    "p = model.predict(data_norm)\n",
    "               \n",
    "y_pred = np.mean(np.square(data_norm - p))\n",
    "if y_pred > decision_threshold:\n",
    "    decision_result = [os.path.basename(file_path), 1]\n",
    "else:\n",
    "    decision_result = [os.path.basename(file_path), 0]\n",
    "print(decision_result)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "a5kAFsPH7IBG",
    "oWUu43o0PLVa",
    "MG4JILM_7xHh",
    "Uyg6SSIm6glJ",
    "M0KJ2Sk9834e",
    "0MKbNMrpPqyS",
    "aqpSEE4sCBvd",
    "9fQzaPI3N1-6"
   ],
   "name": "Alt Carbon – Anomaly Detection For Pumps.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
